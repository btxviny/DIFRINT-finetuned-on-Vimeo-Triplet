{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import os \n",
    "import cv2\n",
    "import warnings\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_warp(image, flow):\n",
    "    \"\"\"\n",
    "    Densely warps an image using optical flow.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (batch_size, channels, height, width).\n",
    "        flow (torch.Tensor): Optical flow tensor of shape (batch_size, 2, height, width).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Warped image tensor of shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    batch_size, channels, height, width = image.size()\n",
    "\n",
    "    # Generate a grid of pixel coordinates based on the optical flow\n",
    "    grid_y, grid_x = torch.meshgrid(torch.arange(height), torch.arange(width))\n",
    "    grid = torch.stack((grid_x, grid_y), dim=-1).to(image.device)\n",
    "    grid = grid.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "    new_grid = grid + flow.permute(0, 2, 3, 1)\n",
    "\n",
    "    # Normalize the grid coordinates between -1 and 1\n",
    "    new_grid /= torch.tensor([width - 1, height - 1], dtype=torch.float32, device=image.device)\n",
    "    new_grid = new_grid * 2 - 1\n",
    "    # Perform the dense warp using grid_sample\n",
    "    warped_image = F.grid_sample(image, new_grid, align_corners=False)\n",
    "\n",
    "    return warped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in DIFRINT model: 11600920\n"
     ]
    }
   ],
   "source": [
    "from models import ResNet, UNet\n",
    "from PWC_src import PWC_Net, flow_to_image\n",
    "from PWC_src.pwc import FlowEstimate\n",
    "class DIFRINT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DIFRINT,self).__init__()\n",
    "        self.resnet = ResNet(hidden_size=64).to(device).eval()\n",
    "        self.unet = UNet(hidden_size=64).to(device).eval()\n",
    "        self.pwc = PWC_Net('./ckpt/sintel.pytorch')\n",
    "        self.pwc.to(device).eval()\n",
    "\n",
    "    def get_flow(self,img1,img2):\n",
    "        img1_t = (img1 + 1) / 2 \n",
    "        img2_t = (img2 + 1) / 2 \n",
    "        flow = FlowEstimate(img1_t,img2_t, self.pwc)\n",
    "        return flow.detach()\n",
    "    \n",
    "    def forward(self, ft_minus, ft, ft_plus):\n",
    "        flo1 = self.get_flow(ft_minus, ft)\n",
    "        flo2 = self.get_flow(ft_plus, ft)\n",
    "        warped1 = dense_warp(ft_minus,0.5 * flo1)\n",
    "        warped2 = dense_warp(ft_plus,0.5 * flo2)\n",
    "        fint = self.unet(warped1, warped2, flo1, flo2, ft_minus, ft_plus)\n",
    "        flo3 = self.get_flow(ft,fint)\n",
    "        warped3 = dense_warp(ft,flo3)\n",
    "        fout = self.resnet(fint, warped3)\n",
    "        return fint, fout\n",
    "\n",
    "difrint = DIFRINT().eval().to(device)\n",
    "total_params = sum(p.numel() for p in difrint.parameters())\n",
    "print(\"Total number of parameters in DIFRINT model: {}\".format(total_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded UNet:unet_2.pth \n",
      "Loaded ResNet:resnet_188.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Load UNet checkpoints\n",
    "unet_path = './ckpts/unet/'\n",
    "ckpts = os.listdir(unet_path)\n",
    "if ckpts:\n",
    "    ckpts = sorted(ckpts, key=lambda x: int(x.split('.')[0].split('_')[1]))\n",
    "    latest = ckpts[-1]\n",
    "    #latest = 'unet_171.pth'\n",
    "    state_dict = torch.load(os.path.join(unet_path, latest))\n",
    "    difrint.unet.load_state_dict(state_dict['model'])\n",
    "    print(f'Loaded UNet:{latest} ')\n",
    "# Load ResNet checkpoints\n",
    "resnet_path = './ckpts/resnet/'\n",
    "ckpts = os.listdir(resnet_path)\n",
    "if ckpts:\n",
    "    ckpts = sorted(ckpts, key=lambda x: int(x.split('.')[0].split('_')[1]))\n",
    "    latest = ckpts[-1]\n",
    "    state_dict = torch.load(os.path.join(resnet_path, latest))\n",
    "    difrint.resnet.load_state_dict(state_dict['model'])\n",
    "    print(f'Loaded ResNet:{latest}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'E:/Datasets/DeepStab_Dataset/unstable/2.avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "hh,ww = 360,640\n",
    "frames = np.zeros((frame_count,hh,ww,3),np.float32)\n",
    "for i in range(frame_count):\n",
    "    ret,img = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    img = cv2.resize(img,(ww,hh))\n",
    "    img = ((img / 255.0) * 2) - 1 \n",
    "    frames[i,...] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "SKIP = 1\n",
    "ITER = 5\n",
    "interpolated = frames.copy()\n",
    "cv2.namedWindow('window',cv2.WINDOW_NORMAL)\n",
    "for iter in range(ITER):\n",
    "    print(iter)\n",
    "    temp = interpolated.copy()\n",
    "    for frame_idx in range(SKIP,frame_count - SKIP):\n",
    "        torch.cuda.empty_cache()\n",
    "        ft_minus = torch.from_numpy(interpolated[frame_idx - SKIP,...]).permute(2,0,1).unsqueeze(0).to(device)\n",
    "        ft = torch.from_numpy(frames[frame_idx]).permute(2,0,1).unsqueeze(0).to(device)\n",
    "        ft_plus = torch.from_numpy(interpolated[frame_idx + SKIP,...]).permute(2,0,1).unsqueeze(0).to(device)\n",
    "        with torch.no_grad(): \n",
    "            fint,fout = difrint(ft_minus,ft,ft_plus)\n",
    "        temp[frame_idx,...] = fout.cpu().squeeze(0).permute(1,2,0).numpy()\n",
    "        img  = (((fout.cpu().squeeze(0).permute(1,2,0).numpy() + 1) / 2)*255.0).astype(np.uint8)\n",
    "        cv2.imshow('window',img)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('9'):\n",
    "            break\n",
    "    interpolated = temp.copy()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "cv2.namedWindow('window',cv2.WINDOW_NORMAL)\n",
    "frame_count, h, w, c = interpolated.shape\n",
    "out_path = f'./2_vimeo.avi'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(out_path, fourcc, 30.0, (w, h))\n",
    "for idx in range(frame_count):\n",
    "    img  = interpolated[idx,...].copy()\n",
    "    img = (((img + 1) /2 ) * 255).astype(np.uint8)\n",
    "    out.write(img)\n",
    "    cv2.imshow('window',img)\n",
    "    #sleep(1/30)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('9'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = './results/comparison2.avi'\n",
    "frame_count, h, w, c = interpolated.shape\n",
    "out_path = name\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(out_path, fourcc, 30.0, (2*w,h))\n",
    "for idx in range(frame_count):\n",
    "    img1  = interpolated[idx,...].copy()\n",
    "    img2 = frames[idx,...].copy()\n",
    "    img1 = (((img1 + 1) /2 ) * 255).astype(np.uint8)\n",
    "    img2 = (((img2 + 1) /2 ) * 255).astype(np.uint8)\n",
    "    conc = cv2.hconcat([img1,img2])\n",
    "    out.write(conc)\n",
    "    cv2.imshow('window',conc)\n",
    "    sleep(1/30)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('9'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "cv2.namedWindow('window',cv2.WINDOW_NORMAL)\n",
    "for idx in range(frame_count):\n",
    "    img  = interpolated[idx,...].copy()\n",
    "    img = (((img + 1) /2 ) * 255).astype(np.uint8)\n",
    "    img1 = frames[idx,...].copy()\n",
    "    img1 = (((img1 + 1) /2 ) * 255).astype(np.uint8)\n",
    "    diff = cv2.absdiff(img,img1)\n",
    "    cv2.imshow('window',diff)\n",
    "    sleep(1/60)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('9'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DUTCode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
